{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "<!-- ; background:url('https://desirschool.sciencesconf.org/data/header/archives_1.jpg')  repeat center center; color: #fff; -->    \n",
    "  <div style=\"flex: 33%;\">\n",
    "      <img src=\"https://desirschool.sciencesconf.org/data/header/DESIR_logo_1.jpg\" width=500>\n",
    "  </div>\n",
    "  <div style=\"flex: 66%; margin: 1em; text-align: center;\">\n",
    "    <h1> DESIR Winter School: Shaping new approaches to data management in arts and humanities </h1>\n",
    "    <h2> Open Research Notebooks </h2>\n",
    "    <h3> 10-13 Dec 2019 Lisbon (Portugal) </h3>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About me\n",
    "<br>\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 50%;\">\n",
    "      <img src=\"https://www.dropbox.com/s/8u2cy57qpz4yx1y/profile_pic.jpg?raw=1\" width=200/>\n",
    "  </div>\n",
    "  <div style=\"flex: 50%;margin: 1em;\">\n",
    "      <b>Javier de la Rosa</b>, <a href=\"mailto:versae@gmail.com\"><i>versae@gmail.com</i></a>, <a href=\"https://twitter.com/versae\"><i>@versae</i></a>\n",
    "      <br />\n",
    "      <br />\n",
    "      <div style=\"padding-left: 1em;\">\n",
    "      Postdoctoral Researcher in NLP at UNED (ERC POSTDATA Project), Spain\n",
    "      <br />\n",
    "      PhD in Hispanic Studies (Digital Humanities), University of Western Ontario, Canada\n",
    "      <br />\n",
    "      Master in Artificial Intelligence, Universidad de Sevilla, Spain\n",
    "      <br />\n",
    "      <br />\n",
    "      Ex-Research Software Engineer at Stanford University, California\n",
    "      <br />\n",
    "      Ex-Technical Lead at the CulturePlex Lab, University of Western Ontario, Canada\n",
    "      </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible Research\n",
    "\n",
    "We'll be focusing on <strong>Mary Chester-Kadwell's <a href=\"https://github.com/mchesterkadwell/bughunt-analysis\" target=\"_blank\">\"text mining of English childrenâ€™s literature 1789-1914 for the representation of insects and other creepy crawlies\"</a></strong>. As such, this notebook is basically an adaption of her MIT licensed notebooks and all credit goes to her."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple String Manipulation in Python\n",
    "This section introduces some very basic things you can do in Python to create and manipulate *strings*. A string is a simple sequence of characters, like `flabbergast`. This introduction is limited to those things that may be useful to know in order to understand the *Bughunt!* data mining in the following two notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Storing Strings in Variables\n",
    "Strings are simple to create in Python. You can simply write some characters in quote marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Butterflies are important as pollinators.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do something useful with this string, other than print it out, we need to store in a *variable* by using the assignment operator `=` (equals sign). Whatever is on the right-hand side of the `=` is stored into a variable with the name on the left-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_variable is the variable on the left\n",
    "# 'manuscripts' is the string on the right that is stored in the variable my_variable\n",
    "\n",
    "my_variable = 'Butterflies are important as pollinators.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that nothing is printing to the screen. That's because the string is stored in the variable `my_variable`. In order to see what is inside the variable `my_variable` we can simply write `my_variable` in a code cell, run it, and the interpreter will print it out for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Bits of Strings\n",
    "\n",
    "#### Accessing Individual Characters\n",
    "A strings is just a sequence (or list) of characters. You can access **individual characters** in a string by specifying which ones you want in square brackets. If you want the first character you specify `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on a minute! Why did it give us `u` instead of `B`?\n",
    "\n",
    "In programming, everything tends to be *zero indexed*, which means that things are counted from 0 rather than 1. Thus, in the example above, `1` gives us the *second* character in the string.\n",
    "\n",
    "If you want the first character in the string, you need to specify the index `0`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a Range of Characters\n",
    "\n",
    "You can also pick out a **range of characters** from within a string, by giving the *start index* followed by the *end index* with a semi-colon (`:`) in between.\n",
    "\n",
    "The example below gives us the character at index `0` all the way up to, *but not including*, the character at index `20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Whole Strings with Functions\n",
    "Python has some built-in *functions* that allow you to change a whole string at once. You can change all characters to lowercase or uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: These functions do not change the original string but create a new one. Our original string is still the same as it was before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Strings\n",
    "\n",
    "You can also test a string to see if it is passes some test, e.g. is the string all alphabetic characters only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_variable.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the string have the letter `p` in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'p' in my_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists of Strings\n",
    "Another important thing we can do with strings is creating a list of strings by listing them inside square brackets `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['Butterflies are important as pollinators',\n",
    "          'Butterflies feed primarily on nectar from flowers',\n",
    "          'Butterflies are widely used in objects of art']\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Lists of Strings\n",
    "Just like with strings, we can access individual items inside a list by index number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can access a range of items inside a list by *slicing*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Creating Lists of Strings with List Comprehensions\n",
    "We can create new lists in an elegant way by combining some of the things we have covered above. Here is an example where we have taken our original list `my_list` and created a new list `new_list` by going over each string in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = [string for string in my_list]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do this? If we combine it with a test, we can have a list that only contains strings with the letter `p` in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_p = [string for string in my_list if 'p' in string]\n",
    "new_list_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very powerful way to quickly create lists. We can even change all the strings to uppercase at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_p_upper = [string.upper() for string in my_list if 'p' in string]\n",
    "new_list_p_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing the Bughunt Corpus\n",
    "This notebook follows the process of taking the manually cleaned Bughunt corpus and creating a frequency distribution of the different bug words.\n",
    "\n",
    "NB: This notebook does not actually process the whole corpus -- that is done by the script `insect-freq-unigram.py`. The examples here are a walk-through and explanation of the code using a single file.\n",
    "\n",
    "We will use the code library called Natural Language Toolkit (NLTK) to provide a lot of text mining functions that are already written. More information on this can be found here: http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Files\n",
    "\n",
    "We already have the corpus **split into files by decade**. Here is a list of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "if not os.path.exists('corpora'):\n",
    "    os.makedirs('corpora')\n",
    "    \n",
    "urls = \"https://raw.githubusercontent.com/mchesterkadwell/bughunt-analysis/master/corpora/bughunt/2-clean-by-decade/bughunt-clean-{}.txt\"\n",
    "for i in range(1800, 1920, 10):\n",
    "    url = urls.format(str(i))\n",
    "    filename = url.rsplit(\"/\", 1)[1]\n",
    "    print(\"Downloading and storing\", filename)\n",
    "    text = requests.get(url).text\n",
    "    with open(Path(\"corpora\", filename), \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Process\n",
    "Before we are ready to process these files, we need to gather together some resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug Words\n",
    "We have our list of **simple bug words** as a text file. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mchesterkadwell/bughunt-analysis/master/wordlists/insect-wordlist.txt\"\n",
    "wordlist = requests.get(url).text\n",
    "with open(Path(\"insect-wordlist.txt\"), \"w\") as f:\n",
    "    f.write(wordlist)\n",
    "bug_words = wordlist.split(\"\\n\")\n",
    "bug_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a list of the **stems** of bug words. **Stemming** is a form of word normalisation. It means reducing a word to its root, eliminating plurals and other inflections. Stems may not be actual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mchesterkadwell/bughunt-analysis/master/wordlists/insect-wordstems.txt\"\n",
    "wordstems = requests.get(url).text\n",
    "with open(Path(\"insect-wordstems.txt\"), \"w\") as f:\n",
    "    f.write(wordstems)\n",
    "bug_stems = wordstems.split(\"\\n\")\n",
    "bug_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the list above, the stems 'butterfli', 'dragonfli' and 'fli' are not real words.\n",
    "\n",
    "This contrasts with **lemmatisation** where the reduced word, the **lemma**, is a proper word in the language; in fact, it is the canonical or dictionary form.\n",
    "\n",
    "### English Stopwords\n",
    "We are not interested in common words in English that carry little meaning, such as 'the', 'a' and 'its'. There is no definitive list of stopwords, but a commonly-used list is provided by the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "!{sys.executable} -m nltk.downloader stopwords\n",
    "# nltk.download('stopwords', download_dir=Path('nltk_data'))\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "sorted(list(english_stops))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the Corpus\n",
    "Tokenising means splitting a text into meaningful elements, such as words, sentences, or symbols.\n",
    "\n",
    "To do this we use a simple facility provided by the NLTK to read in the files and a function to do the tokenising for us. The code example below takes a single corpus file and tokenises it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt', download_dir=Path('..', 'nltk_data'))\n",
    "!{sys.executable} -m nltk.downloader punkt\n",
    "\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "reader = PlaintextCorpusReader('.', '')\n",
    "file_1810 = os.path.join(\"corpora\", 'bughunt-clean-1810.txt')\n",
    "text = reader.raw(file_1810)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of problems with these tokens: the capitalisation of the words has been preserved, and some of the tokens have unwanted special characters or comprise single items of punctuation.\n",
    "\n",
    "### Normalising to Lowercase\n",
    "Normalising all words in a corpus to lowercase ensures that the same word in different cases can be recognised as the same word, e.g. we want 'Gnat', 'gnat' and 'GNAT' to be recognised as the same word.\n",
    "\n",
    "However, whether you choose to do this depends on the nature of your corpus and the questions you are investigating. For example, in another case, you may be not want the word 'Conservative' to be conflated with the word 'conservative'.\n",
    "\n",
    "In our case, we will lowercase the whole corpus immediately before tokenising it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Puctuation\n",
    "Punctuation such as commas, fullstops and apostrophes can complicate processing a corpus. For example, if punctuation is left in, the words \"termite\" and \"termite,\" might be considered to be different words.\n",
    "\n",
    "This is a complicated matter, however, and what you choose to do would vary depending on the nature of your corpus and what questions you wish to ask.\n",
    "\n",
    "It may be appropriate to remove punctuation at different stages of processing. In our case we are going to remove it *after* the text has been tokenised.\n",
    "\n",
    "We will replace *all* punctuation with the empty string ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "tokens_nopunct = [token.translate(table) for token in tokens]\n",
    "tokens_nopunct[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-Word Tokens\n",
    "\n",
    "We are still left with some problematic tokens that are not useful words, such as empty tokens `''` and tokens that may be chapter numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_empty = [word for word in tokens_nopunct if not word.isalpha()]\n",
    "tokens_empty[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nonwords = [word for word in tokens_nopunct if word.isnumeric()]\n",
    "tokens_nonwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove both these by filtering for only those words that are alphabetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in tokens_nopunct if word.isalpha()]\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "We are now ready to remove the stopwords we prepared earlier and thereby create a list of only meaningful words. Before using the stopwords, we will also remove all the punctuation so that it matches the text of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops_nopunct = {stopword.translate(table) for stopword in english_stops}\n",
    "words_nostops = [word for word in words if word not in english_stops_nopunct]\n",
    "words_nostops[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the Tokens\n",
    "Stemming the tokens ensures that plurals and adjectives are reduced to the same stem and can be counted as the same word. For example, 'lice' and 'louse' will be normalised to 'lous', but so too will 'lousy', which may or may not be desirable.\n",
    "\n",
    "To do this we use another facility provided by the NLTK called a **stemmer**. There are many different ways to stems words, but we will use the Porter Stemmer. (The Porter Stemmer is the original stemmer, first created in 1979. It is simple and speedy, but has some important limitations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(word) for word in words_nostops]\n",
    "stems[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Frequency Distribution\n",
    "At last, we are ready to create a frequency distribution. We will use another NLTK facility called `FreqDist` to count the frequency of each unique word in the corpus, and then create a relative frequency value between `0` and `1`.\n",
    "\n",
    "First, we create a frequency distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freqdist = FreqDist(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 20 most frequent words (the numbers are the absolute word count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not interested in a lot of these words, so the next thing to do is filter out all the words that are not in our list of bugs. Once we have done this we have a dictionary of stems and their relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "insect_words = WordListCorpusReader('.', [Path('insect-wordstems.txt')])\n",
    "\n",
    "insect_freq = {word: freqdist.freq(word) for word in insect_words.words()}\n",
    "insect_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "In the script `insect-freq-unigram.py` the process above is applied to each of the corpus files in turn, and the results are output as the CSV file `insect-stem-freq-unigram.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/insect-freq-unigram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the Bughunt Corpus\n",
    "This notebook follows the process of taking the frequency distribution of the different bug words and creating a visualisation of how frequency changes over time.\n",
    "\n",
    "NB: This notebook does not actually create the figure `insect-stem-freq-unigram.png` -- that is done by the script `insect-freq-unigram.py`. The examples here are a walk-through and explanation of the code.\n",
    "\n",
    "We will use the code library called Natural Language Toolkit (NLTK) to provide a lot of text mining functions that are already written. More information on this can be found here: http://www.nltk.org/. We will also use two popular libraries: Pandas for data manipulation (https://pandas.pydata.org/) and matplotlib (https://matplotlib.org/) to create the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "### Loading the CSV File\n",
    "\n",
    "After text processing the corpus, the results were saved as a CSV file. First we have to load the data from this file into what is called a 'dataframe', which is much like a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path('insect-stem-freq-unigram.csv')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(data_path, index_col='year').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Bug Word list\n",
    "We also need the bug word list again from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "stemlist = Path('insect-wordstems.txt')\n",
    "\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "insect_words = WordListCorpusReader('.', [stemlist])\n",
    "insect_words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Now that we have the data ready, we can experiment with some plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cycler import cycler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.style.use('fivethirtyeight')\n",
    "cc = (cycler(color=['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', \n",
    "                    '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', \n",
    "                    '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', \n",
    "                    '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080',]) *\n",
    "      cycler(linestyle=['-']))\n",
    "plt.rc('axes', prop_cycle=cc)\n",
    "\n",
    "ax = plt.gca()\n",
    "for insect in insect_words.words():\n",
    "    df.plot(kind='line', y=insect, ax=ax)\n",
    "\n",
    "plt.axis([1800, 1910, 0, 0.009])\n",
    "plt.xticks(np.arange(1800, 1920, 10))\n",
    "plt.ylabel('frequency of bug stem')\n",
    "plt.suptitle('Frequency of Bugs in Children\\'s Literature by Decade 1800-1920')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
